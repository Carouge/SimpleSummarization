{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "import os, sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All papers without abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_abstract(words):\n",
    "    \n",
    "    start, end, intro = [], [], []\n",
    "    \n",
    "    for word_id in range(len(words)):\n",
    "        if 'abstract' in words[word_id].lower():\n",
    "            start.append(word_id)\n",
    "        elif 'keywords' in words[word_id].lower():\n",
    "            end.append(word_id)\n",
    "        elif 'introduction' in words[word_id].lower():\n",
    "            intro.append(word_id)\n",
    "    \n",
    "    if end[0] == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    if intro[0] < end[0]:\n",
    "        end_point = intro[0]\n",
    "    else:\n",
    "        end_point = end[0]\n",
    "    return ''.join(words[start[0]+1:end_point]), end_point\n",
    "\n",
    "\n",
    "def get_text(words):\n",
    "    abstract, text_start = get_abstract(words)\n",
    "    if abstract == 0:\n",
    "        return 0, 0\n",
    "    text = ''.join(words[text_start+1:])\n",
    "    return abstract, text\n",
    "\n",
    "\n",
    "def parse_all_pdf(path):\n",
    "    all_pdf = pd.DataFrame(columns=['name', 'abstract', 'text'])\n",
    "    ind = 0\n",
    "    for f in os.listdir(path):\n",
    "        parsedPDF = parser.from_file(path+f)\n",
    "        try:\n",
    "            pdf = ' '.join(parsedPDF['content'].split())\n",
    "            words, word = [], []\n",
    "            for st in pdf:\n",
    "                word.append(st)\n",
    "                if st == ' ':\n",
    "                    words.append(''.join(word))\n",
    "                    word = []\n",
    "            abstract, text = get_text(words)\n",
    "            \n",
    "            if abstract == 0 or len(abstract) > 1500 :\n",
    "                pass\n",
    "            else:\n",
    "                all_pdf.loc[ind] = [f, abstract, text]\n",
    "                ind += 1\n",
    "                print(ind)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "            \n",
    "    all_pdf.to_csv('data.csv', index=False)\n",
    "    return \"DONE!\"\n",
    "        \n",
    "\n",
    "#################  Extract text #####################\n",
    "\n",
    "abs_path = '/home/maria/Documents/Courses_UCU/ML/Course_work/stat.ML-2K/papers/'\n",
    "\n",
    "parse_all_pdf(abs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Parsing single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '/home/maria/Documents/Courses_UCU/ML/Course_work/stat.ML-2K/papers/0708.2377v1.pdf'\n",
    "parsedPDF = parser.from_file(path)\n",
    "pdf = ' '.join(parsedPDF['content'].split())\n",
    "words, word = [], []\n",
    "for st in pdf:\n",
    "    word.append(st)\n",
    "    if st == ' ':\n",
    "        words.append(''.join(word))\n",
    "        word = []\n",
    "text = ''.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arXiv:0708.2377v1 [stat.ML] 17 Aug 2007 ar X iv :0 70 8. 23 77 v1 [ st at .M L ] 1 7 A ug 2 00 7 Online Learning in Discrete Hidden Markov Models Roberto Alamino∗ and Nestor Caticha† ∗Neural Computing Research Group, Aston University Aston Triangle, Birmingham, B4 7ET, United Kingdom †Instituto de Física, Universidade de São Paulo, CP 66318 São Paulo, SP, CEP 05389-970 Brazil Abstract. We present and analyse three online algorithms for learning in discrete Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm. Using the Kullback-Leibler divergence as a measure of generalisation error we draw learning curves in simplified situations. The performance for learning drifting concepts of one of the presented algorithms is analysed and compared with the Baldi-Chauvin algorithm in the same situations. A brief discussion about learning and symmetry breaking based on our results is also presented. Key Words: HMMs, Online Algorithm, Generalisation Error, Bayesian Algorithm. INTRODUCTION Hidden Markov Models (HMMs) [1, 2] are extensively studied machine learning models for time series with several applications in fields like speech recognition [2], bioinfor- matics [3, 4] and LDPC codes [5]. They consist of a Markov chain of non-observable hidden states qt ∈ S, t = 1, ...,T , S = {s1,s2, ...,sn}, with initial probability vector πi = P(q1 = si) and transition matrix Aij(t) = P(qt+1 = sj |qt = si), i, j = 1, ..,n. At discrete times t, each qt emits an observed state yt ∈ O, O = {o1, ...,om}, with emis- sion probability matrix Biα(t) = P(yt = oα|qt = si), i= 1, ...,n, α = 1, ...,m, which are the actual observations of the time series represented, from time t = 1 to t = T , by the observed sequence yT1 = {y1,y2, ...,yT}. The qt’s form the so called hidden sequence qT1 = {q1, q2, ..., qT}. The probability of observing a sequence y T 1 given ω ≡ (π,A,B) is P(yT1 |ω) = ∑ qT 1 P(y1)P(y1|q1) T ∏ t=2 P(qt+1|qt)P(yt|qt). (1) In the learning process, the HMM is fed with a series and adapts its parameters to produce similar ones. Data feeding can range from offline (all data is fed and parameters calculated all at once) to online (data is fed by parts and partial calculations are made). We study a scenario with data generated by a HMM of unknown parameters, an ex- tension of the student-teacher scenario from neural networks. The performance, as a function of the number of observations, is given by how far, measured by a suitable cri- http://arXiv.org/abs/0708.2377v1 terion, is the student from the teacher. Here we use the naturally arising Kullback-Leibler (KL) divergence that, although not accessible in practice since it needs knowledge of the teacher, is an extension of the idea of generalisation error being very informative. We propose three algorithms and compare them with the Baldi-Chauvin Algorithm (BC) [6]: the Baum-Welch Online Algorithm (BWO), an adaptation of the offline Baum- Welch Reestimation Formulas (BW) [1] and, starting from a Bayesian formulation, an approximation named Bayesian Online Algorithm (BOnA), that can be simplified again without noticeable lost of performance to a Mean Posterior Algorithm (MPA). BOnA and MPA, inspired by Amari [7] and Opper [8], are essentially mean field methods [9] in which a manifold of prior tractable distributions is introduced and the new datum leads, through Bayes theorem, to a non-tractable posterior. The key step is to take as the new prior, not the posterior, but the closest distribution (in some sense) in the manifold. The paper is organised as follows: first, BWO is introduced and analysed. Next, we derive BOnA for HMMs and, from it, MPA. We compare MPA and BC for drifting con- cepts. Then, we discuss learning and symmetry breaking and end with our conclusions. BAUM-WELCH ONLINE ALGORITHM The Baum-Welch Online Algorithm (BWO) is an online adaptation of BW where in each iteration of BW, y becomes yp, the p-th observed sequence. Multiplying the BW increment by a learning rate ηBW we get the update equations for ω ω̂p+1 = ω̂p +ηBW ∆̂ω p, (2) with ∆̂ωp the BW variations for yp. The complexity of BWO is polynomial in n and T . In figure 1, the HMM learns sequences generated by a teacher with n= 2, m= 3 and T = 2 for different ηBW . Initial students have matrices with all entries set to the same value, what we call a symmetric initial student. We took averages over 500 random teachers and distances are given by the KL-divergence between two HMMs ω1 and ω2 dKL(ω1,ω2) ≡ ∑ yT 1 P(yT1 |ω1) ln [ P(yT1 |ω1) P(yT1 |ω2) ] . (3) We see that after a certain number of sequences the HMM stops learning, which is particular to the symmetric initial student and disappears for a non-symmetric one. Denoting the variation of the parameters in BC by ∆, in BW by ∆̂, in BWO by ∆̃, and with γt(i) ≡ P(qt = si|yp,ωp), we have to first order in λ ∆πi = ληBC n ∆̂πi = λ n ηBC ηBW ∆̃πi, (4) ∆Aij = ληBC n [ T−1 ∑ t=1 γt(i) ] ∆̂Aij = λ n ηBC ηBW [ T−1 ∑ t=1 γt(i) ] ∆̃Aij , ∆Biα = ληBC n [ T ∑ t=1 γt(i) ] ∆̂Biα = λ n ηBC ηBW [ T ∑ t=1 γt(i) ] ∆̃Biα. 10 100 1000 p 0.1 d (K ul lb ac k- L ei bl er ) 0.001 0.01 0.1 FIGURE 1. Log-log curves of BWO for three different ηBW indicated next to the curves. For ηBW ≈ ληBC/n and small λ, variations in BC are proportional to those in BWO, but with different effective learning rates for each matrix depending on yp. Simulations show that actual values are of the same order of approximated ones. THE BAYESIAN ONLINE ALGORITHM The Bayesian Online Algorithm (BOnA) [8] uses Bayesian inference to adjust ω in the HMM using a data set DP = {y1, ...,yP}. For each data, the prior distribution is updated by Bayes’ theorem. This update takes a prior from a parametric family and transforms it in a posterior which in general has no longer the same parametric form. The strategy used by BOnA is then to project the posterior back into the initial parametric family. In order to achieve this, we minimise the KL-divergence between the posterior and a distribution in the parametric family. This minimisation will enable us to find the parameters of the closest parametric distribution by which we will approximate our posterior. The student HMM ω parameters in each step of the learning process are estimated as the means of the each projected distribution. For a parametric family that has the form P (x) ∝ e− P i λifi(x), which can be obtained by the MaxEnt principle where we constrain the averages over P (x) of arbitrary func- tions fi(x), minimising the KL-divergence turns out to be equivalent to equating the averages < fi(x) > over P (x) to the average of these functions over the unprojected posterior (our posterior distribution just after the Bayesian update for the next data). For HMMs, the vector π and each i-th row Ai of A and Bi of B are different discrete distributions which we assume independent in order to write the factorized distribution P(ω|u) ≡P(π|ρ) n ∏ i=1 P(Ai|ai)P(Bi|bi), (5) where u= (ρ,a,b) represents the parameters of the distributions. As each factor is a distribution over probabilities, the natural choice are the Dirichlet distributions, which for a N-dimensional variable x is D(x|u) = Γ(u0) ∏N i=1 Γ(ui) N ∏ i=1 xui−1i , (6) where u0 = ∑ iui and Γ is the analytical continuation of the factorial to real numbers. These can be obtained from MaxEnt with fi(x) = lnxi [13]: ∫ dµD(x) lnxi = αi, dµ≡ δ ( ∑ i xi−1 ) ∏ i θ(xi)dxi. (7) The function to be extremized is L = ∫ dµD lnD+λ ( ∫ dµD−1 ) + ∑ i λi ( ∫ dµD lnxi−αi ) , (8) and with δL/δD = 0 we get the Dirichlet with normalisation eλ+1 and ui = 1−λi. Each factor distribution is separately projected by equating the average of the loga- rithms in the original posterior Q and in the projected distributions ψ(ρi)−ψ ( ∑ j ρj ) = 〈lnπi〉Q ≡ µi(ρ), (9) ψ(aij)−ψ ( ∑ k aik ) = 〈lnAij〉Q ≡ µij(a), ψ(biα)−ψ ( ∑ β biβ ) = 〈lnBiα〉Q ≡ µiα(b), where ψ(x) = d lnΓ(x)/dx is the digamma function. We call a set of N equations ψ(xi)−ψ ( ∑ j xj ) = µi, (10) with i= 1, ...N a digamma system in the variables xi with coefficients µi. Let us call P p(ω) the projected distribution after observation of yp, and Qp+1(ω) the posterior distribution (not projected yet) after yp+1. By Bayes’ theorem, Qp+1(ω) ∝ P p(ω) ∑ qp+1 P(yp+1, qp+1|ω). (11) The calculation of µ’s in (9) leads to averages over Dirichlets of the form [10] µi = 〈[ ∏ j x rj j ] lnxi 〉 = Γ(u0) ∏ j Γ(uj) ∏ j Γ(uj + rj) Γ(u0 + r0) [ψ(ui + ri)−ψ(u0 + r0)]. (12) To solve (10), we solve for xi, sum over i with x0 ≡ ∑ ixi and find numerically, by iterating from an arbitrary initial point, the fixed points of the one-dimensional map xn+10 = ∑ i ψ−1[µi +ψ(x n 0 )], (13) 1 10 100 p 0.1 d (K ul lb ac k- L ei bl er ) FIGURE 2. Comparison in log-log scale of MPA (dashed line) and BOnA (circles). where we found a unique solution except for µi ≈ 0, which is rare in most applications. BOnA has a common problem of Bayesian algorithms: the sum over hidden vari- ables makes the complexity scales exponentially in T . Also, the calculation of several digamma functions is very time consuming. In the following, we develop an approxi- mation that runs faster, although still with exponential complexity in T . This is not a problem for we can make T constant and the algorithm will scale polynomially in n. MEAN POSTERIOR APPROXIMATION The Mean Posterior Approximation (MPA) is a simplification of BOnA inspired in its results for Gaussians, where we match first and second moments of posterior and projected distributions. Noting it, instead of minimising dKL we match the mean and one of the variances of posterior and projected distributions as an approximation, which gives, with hatted variables for reestimated values [10] ρ̂i = 〈πi〉Q 〈π1〉Q −〈π 2 1〉Q 〈π21〉Q−〈π1〉 2 Q , (14) âij = 〈aij〉Q 〈ai1〉Q −〈a 2 i1〉Q 〈a2i1〉Q −〈ai1〉 2 Q , b̂iα = 〈biα〉Q 〈bi1〉Q−〈b 2 i1〉Q 〈b2i1〉Q−〈bi1〉 2 Q , with complexity again of order nT , but with heavily reduced real computational time making it better for practical applications. Figure 2 compares MPA and BOnA. The initial difference decreases in time and both come closer relatively fast. We used n = 2, m = 3 and T = 2 and averaged over 150 random teachers with symmetric initial students. The computational time for BOnA was 340min, and for MPA, 5s in a 1GHz processor. Figure 3a compares MPA to BC and figure 3b to BWO. In both cases MPA has better generalisation. We used n = 2, m= 3, T = 2, symmetric initial students and averaged over 500 random teachers. 10 100 1000 10000 p 0.1 d (K ul lb ac k- L ei bl er ) 0.001 0.01 0.1 10 100 1000 10000 p 0.1 d (K ul lb ac k- L ei bl er ) 0.005 0.0001 0.0005 a) b) FIGURE 3. a) Comparison between MPA (dashed) and BC (continuous). Values of λ are indicated next to the curves. ηBC = 0.5. b) Comparison between MPA (dashed) and BWO (continuous). Values of ηBW are indicated next to the curves. Both scales are log-log. 0 500 1000 1500 2000 2500 3000 p 0 0.2 0.4 0.6 0.8 d (K ul lb ac k- L ei bl er ) 0 500 1000 1500 2000 2500 3000 p 0.2 0.4 0.6 0.8 1 d (K ul lb ac k- Le ib le r) a) b) FIGURE 4. Drifting concepts. Continuous lines correspond to MPA and dashed lines to BC. a) Abrupt changes at 500 sequences interval. b) Small random changes at each new sequence. LEARNING DRIFTING CONCEPTS We tested BC and MPA for changing teachers. In figure 4a, it changes at random after each 500 sequences (λ = 0.01, ηBC = 10.0). In figure 4b, each time a sequence is observed, a small random quantity is added to the teacher. Both have n = 2, m = 3 and are averaged over 200 runs. Figure 4b shows that BC adapts better, but is not fully adaptive and we do not know how to modify it. MPA instead derives from Bayesian principles and we can guess the problem by analogy with similar Bayesian algorithms [12]: variances decrease in the process as in the perceptron, where they are the learning rates, explaining the memory effect difficulting the learning after changes. Although not proved yet, we expect the same relationship in MPA, which can be used to improve performance. LEARNING AND SYMMETRY BREAKING Learning from symmetric initial students requires that the parameters separate from each other in some point, which depends on the algorithm and is an important feature in online 0 1000 2000 3000 p 0 0.5 1 1.5 2 2.5 3 d ( K u ll b ac k -L ei b le r) 0 1000 2000 3000 p 0 0.2 0.4 0.6 0.8 1 In it ia l P ro b ab il it ie s 0 1000 2000 3000 p 0 0.2 0.4 0.6 0.8 1 T ra n si ti o n M at ri x 0 1000 2000 3000 p 0 0.2 0.4 0.6 0.8 1 E m is si o n M at ri x 0 1000 2000 3000 p 0 0.5 1 1.5 2 2.5 3 d ( K u ll b ac k -L ei b le r) 0 1000 2000 3000 p 0 0.2 0.4 0.6 0.8 1 In it ia l p ro b ab il it ie s 0 1000 2000 3000 p 0 0.2 0.4 0.6 0.8 1 T ra n si ti o n M at ri x 0 1000 2000 3000 p 0 0.2 0.4 0.6 0.8 1 E m is si o n M at ri x a) b) FIGURE 5. KL-divergence and student’s parameters for a) BC and b) MPA. algorithms [11], breaking the symmetry with a sharp decrease in the generalisation error. Instead of taking averages to smooth abrupt changes, here we draw curves for only one teacher, rendering them visible. Flat lines before a symmetry breaking are called plateaux and occur when it is difficult to break the symmetry. Figure 5a shows BC (λ= 0.01, ηBC = 1.0) with two abrupt changes: in the beginning and after 1000 sequences. π and A only break the symmetry in the second point, and B in both. Figure 5b shows that in MPA the second change is stronger and the symmetry breaking affects both B and A. Figure 6 shows BWO with ηBW = 0.01 where only B is affected. The more symmetries are broken, the best the generalisation of the algorithm. In all simulations we set n= 2, m= 3 and T = 2 with a teacher HMM given by π = ( 1 0 ) , A = ( 0 1 1 0 ) , B = ( 1 0 0 0 0 1 ) . (15) CONCLUSIONS We proposed and analysed three learning algorithms for HMMs: Baum-Welch On- line (BWO), Bayesian Online Algorithm (BOnA) and Mean Posterior Approximation (MPA). We showed the superior performance of MPA for static teachers, but the Baldi- Chauvin (BC) algorithm is better for drifting concepts, although the Bayesian nature of MPA suggests how to fix it. The results seem to be confirmed by initial tests on real data. The importance of symmetry breaking in learning processes is presented here in a brief discussion where the phenomenon is shown to occur in our models. ACKNOWLEDGEMENTS We would like to thank Evaldo Oliveira, Manfred Opper and Lehel Csato for useful discussions. This work was made part in the University of São Paulo with financial support of FAPESP and part in the Aston University with support of Evergrow Project. 0 1000 2000 3000 p 1 1.5 2 2.5 3 d (K ul lb ac k- L ei bl er ) 0 1000 2000 3000 p 0.2 0.4 0.6 0.8 1 In iti al p ro ba bi lit ie s 0 1000 2000 3000 p 0.2 0.4 0.6 0.8 1 T ra ns iti on M at ri x 0 1000 2000 3000 p 0 0.1 0.2 0.3 0.4 0.5 E m is si on M at ri x FIGURE 6. KL-divergence and student’s parameters for BWO. REFERENCES 1. Y. Ephraim, N. Merhav, Hidden Markov Processes. IEEE Trans. Inf. Theory 48, 1518-1569 (2002). 2. L. R. Rabiner, A Tutorial on Hidden Markov Models and Selected Applications in Speech Recogni- tion. Proc. IEEE 77, 257-286 (1989). 3. P. Baldi, S. Brunak, Bioinformatics: The Machine Learning Approach. MIT Press (2001). 4. R. Durbin, S. Eddy, A. Krogh, G. Mitchison, Biological sequence analysis: Probabilistic models of proteins and nucleic acids. Cambridge University Press, Cambridge (1998). 5. J. Garcia-Frias, Decoding of Low-Density Parity-Check Codes Over Finite-State Binary Markov Channels. IEEE Trans. Comm. 52, 1840-1843 (2004). 6. P. Baldi, Y. Chauvin, Smooth On-Line Learning Algorithms for Hidden Markov Models. Neural Computation 6, 307-318 (1994). 7. S. Amari, Neural learning in structured parameter spaces - Natural Riemannian gradient. NIPS’96 9, MIT Press (1996). 8. M. Opper, A Bayesian Approach to On-line Learning. On-line learning in Neural Networks, edited by D. Saad, Publications of the Newton Institute, Cambridge Press, Cambridge (1998). 9. M. Opper, D. Saad, Advanced Mean Field Methods: Theory and Practice. MIT Press (2001). 10. R. Alamino, N. Caticha, Bayesian Online Algorithms for Learning in Discrete Hidden Markov Models. Submitted to Discrete and Continuous Dynamical Systems. 11. T. Heskes, W. Wiegerinck, W., On-line Learning with Time-Correlated Examples. On-line Learning in Neural Networks, 251-278, edited by David Saad, Cambridge University Press, Cambridge (1998). 12. R. Vicente, O. Kinouchi, N. Caticha. Statistical Mechanics of Online Learning of Drifting Concepts: A Variational Approach. Machine Learning 32, 179-201 (1998). 13. M. O. Vlad, M. Tsuchiya, P. Oefner, J. Ross. Bayesian analysis of systems with random chemical composition: Renormalization-group approach to Dirichlet distributions and the statistical theory of dilution. Phys. Rev. E 65, 011112(1)-01112(8) '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_text = ''\n",
    "for char in text:\n",
    "    if type(char) == str:\n",
    "        str_text += char\n",
    "str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('./data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(all_data['abstract'].isnull() == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boosting; weak learners; Kernel-based methods; Reproducing kernel Hilbert spaces; robust estimation 1. Introduction Boosting is a popular technique to construct learning algorithms (Schapire, 2003). The basic idea is that any weak learner, i.e. algorithm that is only slightly better than guessing, can be used to build an effective learning mechanism that achieves high accuracy. Since the introduction of boosting in Schapire’s seminal work (Schapire, 1990), numerous vari- ants have been proposed for regression, classification, and specific applications including semantic learning and computer vision (Schapire and Freund, 2012; Viola and Jones, 2001; Temlyakov, 2000; Tokarczyk et al., 2015; Bissacco et al., 2007; Cao et al., 2014). In partic- ular, in the context of classification, LPBoost, LogitBoost (Friedman et al., 2000), Bagging c©20XX Aleksandr Y. Aravkin, Giulio Bottegal, and Gianluigi Pillonetto. ar X iv :1 60 8. 02 48 5v 2 [ st at .M L ] 1 3 A pr 2 01 7 Aravkin, Bottegal and Pillonetto and Boosting (Lemmens and Croux, 2006) and AdaBoost (Freund and Schapire, 1997) have become standard tools, the latter having being recognized as the best off-the-shelf binary classification method (Breiman, 1998; Zhu et al., 2009). Applications of the boosting prin- ciple are also found in decision tree learning (Tu, 2005) and distributed learning (Fan et al., 1999). For a survey on applications of boosting in classification tasks see the work of Fre- und et al. (1999). For regression problems, AdaBoost.RT (Solomatine and Shrestha, 2004; Avnimelech and Intrator, 1999) and `2 Boost (Bühlmann and Yu, 2003; Tutz and Binder, 2007; Champion et al., 2014) are the most prominent boosting algorithms. In particular, in `2 boosting the weak learner often corresponds to a kernel-based estimator with a heav- ily weighted regularization term. The fit on the training set is then measured using the quadratic loss and increases at each iteration. Hence, the procedure can lead to overfitting if it continues too long (Bühlmann and Hothorn, 2007). To avoid this, several stopping criteria based on model complexity arguments have been developed. Hurvich et al. (1998) propose a modified version of Akaike’s information criterion (AIC); Hansen and Yu (2001) use the principle of minimum description length (MDL), and Bühlmann and Yu (2003) suggest a five-fold cross validation. In this paper, we focus on `2 boosting and consider linear weak learners induced by the combination of a quadratic loss and a regularizer induced by a kernel K. We show that the resulting boosting estimator is equivalent to estimation with a special boosting kernel that depends on K, as well as on the regression matrix, noise variance, and hyperparame- ters. This viewpoint leads to both greater generality and better computational efficiency. In particular, the number of boosting iterations ν is a continuous hyperparameter of the boosting kernel, and can be tuned by standard fast hyper-parameter selection techniques including SURE, generalized cross validation, and marginal likelihood (Hastie et al., 2001a). In Section 5, we show that tuning ν is far more efficient than applying boosting iterations, and non-integer values of ν can improve performance. We then generalize the boosting kernel to a wider class of problems, including robust regres- sion, by combining the boosting kernel with piecewise linear quadratic (PLQ) loss functions (e.g. `1, Vapnik, Huber). The computational burden of standard boosting is high for gen- eral loss functions, since the estimator at each iteration is no longer a linear function of the data. The boosting kernel makes the general approach tractable. We also use the boost- ing kernel in the context of regularization problems in reproducing kernel Hilbert spaces (RKHSs), e.g. to solve classification formulations that use the hinge loss. The organization of the paper is as follows. After a brief overview of boosting in regres- sion and classification, we develop the main connection between boosting and kernel-based methods in the context of finite-dimensional inverse problems in Section 2. Consequences of this connection are presented in Section 3. In Section 4 we combine the boosting kernel with PLQ penalties to develop a new class of boosting algorithms. We also consider regression and classification in RKHSs. In Section 5 we show numerical results for several experiments involving the boosting kernel. We end with discussion and conclusions in Section 6. 2. Boosting as a kernel-based method In this section, we give a basic overview of boosting, and present the boosting kernel. 2 Boosting as a Kernel-Based Method 2.1 Boosting: notation and overview Assume we are given a model g(θ) for some observed data y ∈ Rn, where θ ∈ Rm is an unknown parameter vector. Suppose our estimator θ̂ for θ minimizes some objective that balances variance with bias. In the boosting context, the objective is designed to provide a weak estimator, i.e. one with low variance in comparison to the bias. Given a loss function V and a kernel matrix K ∈ Rm×m, the weak estimator can be defined by minimizing the regularized formulation θ̂ := arg min θ { J(θ; y) := V(y − g(θ)) + γθTK−1θ } , (1) where the regularization parameter γ is large and leads to over-smoothing. Boosting uses this weak estimator iteratively, as detailed below. The predicted data for an estimator θ̂ are denoted by ŷ = g(θ̂). Boosting scheme: 1. Set ν = 1 and obtain θ̂(1) and ŷ(1) = g(θ̂(1)) using (1); 2. Solve (1) using the current residuals as data vector, i.e. compute θ̂(ν) = argmin θ J(θ; y − ŷ(ν)), and set the new predicted output to ŷ(ν + 1) = ŷ(ν) + g(θ̂(ν)). 3. Increase ν by 1 and repeat step 2 for a prescribed number of iterations. 2.2 Using regularized least squares as weak learner Suppose data y are generated according to y = Uθ + v, v ∼ N (0, σ2I), (2) where U is a known regression matrix of full column rank. The components of v are independent random variables, mean zero and variance σ2. We now use a quadratic loss to define the regularized weak learner. Let λ to denote the kernel scale factor and set γ = σ2/λ so that (1) becomes θ̂ = arg min θ ‖y − Uθ‖2 + σ 2 λ θTK−1θ. (3) We obtain the following expressions for the predicted data ŷ = Uθ̂: ŷ = arg min f { ‖y − f‖2 + σ2fTP−1λ f } = Pλ(Pλ + σ 2I)−1y, (4) where Pλ = λUKU T (5) 3 Aravkin, Bottegal and Pillonetto is assumed invertible for the moment. This assumption will be relaxed later on. The following well-known connection (Wahba, 1990) between (3) and Bayesian estima- tion is useful for theoretical development. Assume that θ and v are independent Gaussian random vectors with priors θ ∼ N (0, λK), v ∼ N (0, σ2I). Then, (3) and (4) provide the minimum variance estimates of θ and Uθ conditional on the data y. In view of this, we refer to diagonal values of K as the prior variances of θ. 2.3 The boosting kernel Define Sλ = Pλ(Pλ + σ 2I)−1. (6) Fixing a small λ, the predicted data obtained by the weak kernel-based learner is ŷ(ν = 1) = Sλy, where ν is the number of boosting iterations. According to the scheme specified in Section 2.1, as ν increases, boosting refines the estimate as follows: ŷ(2) = Sλy + Sλ(I − Sλ)y ŷ(3) = Sλy + Sλ(I − Sλ)y + Sλ(I − Sλ)2y ... ŷ(ν) = Sλ ν−1∑ i=0 (I − Sλ)i y. (7) We now show that the boosting estimates ŷ(ν) are kernel-based estimators from the boosting kernel, which plays a key role for subsequent developments. Proposition 1 The quantity ŷ(ν) is a kernel-based estimator ŷ(ν) = Sλ,νy = Pλ,ν(Pλ,ν + σ 2I)−1y, where Pλ,ν is the boosting kernel defined by Pλ,ν = σ 2 ( I − Pλ ( Pλ + σ 2I )−1)−ν − σ2I = σ2 (I − Sλ)−ν − σ2I. (8) Proof First note that Sλ satisfies Sλ = Pλ ( Pλ + σ 2I )−1 = I − σ2 ( Pλ + σ 2I )−1 . (9) This follows simply from adding the term σ2 ( Pλ + σ 2 )−1 to (6) and observing that expres- sion reduces to I. Next, plugging in the expression (8) for Pλ,ν into the right hand side of 4 Boosting as a Kernel-Based Method expression (9) for Sλ,ν , we have Sλ,ν = I − σ2 ( Pλ,ν + σ 2I )−1 = I − σ2 ( σ2 (I − Sλ)−ν )−1 = I − (I − Sλ)ν = Sλ ν−1∑ i=0 (I − Sλ)i , exactly as required by (7). In Bayesian terms, for a given ν, the above result also shows that boosting returns the mini- mum variance estimate of the noiseless output f conditional on y if f and v are independent Gaussian random vectors with priors f ∼ N (0, Pλ,ν), v ∼ N (0, σ2I). (10) 3. Consequences In this section, we use Proposition 1 to gain new insights on boosting and a new perspective on hyperparameter tuning. 3.1 Insights on the nature of boosting We first derive a new representation of the boosting kernel Pλ,ν via a change of coordinates. Let V DV T be the SVD of UKUT . Then, we obtain Pλ,v = σ2 (σ2)ν ( λUKUT + σ2I )ν − σ2I = σ2V [( λD + σ2I σ2 )ν − I ] V T (11) and the predicted output can be rewritten as ŷ(ν) = V ( I − σ2ν ( λD + σ2I )−ν) V T y. In coordinates z = V T y, the estimate of each component of z is ẑi(ν) = ( 1− σ 2ν( λd2i + σ 2 )ν ) zi, (12) and corresponds to the regularized least squares estimate induced by a diagonal kernel with (i, i) entry σ2 ( λd2i σ2 + 1 )ν − σ2. (13) In Bayesian terms, (13) is the prior variance assigned by boosting to the noiseless output V TUθ. 5 Aravkin, Bottegal and Pillonetto Eq. (13) shows that boosting builds a kernel on the basis of the output signal-to-noise ratios SNRi = λd2i σ2 , which then enter ( λd2i σ2 + 1 )ν . All diagonal kernel elements with di > 0 grow to ∞ as ν increases; therefore asymptotically, data will be perfectly interpolated but with growth rates controlled by the SNRi. If SNRi is large, the prior variance in- creases quickly and after a few iterations the estimator is essentially unbiased along the i-th direction. If SNRi is close to zero, the i-th direction is treated as though affected by ill-conditioning, and a large ν is needed to remove the regularization on ẑi(ν). This perspective makes it clear when boosting can be effective. In the context of inverse problems (deconvolution), θ in (2) represents the unknown input to a linear system whose impulse response defines the regression matrix U . For simplicity, assume that the kernel K is set to the identity matrix, so that the weak learner (3) becomes ridge regression and the d2i in (13) reflect the power content of the impulse response at different frequencies. Then, boosting can outperfom standard ridge regression if the system impulse response and input share a similar power spectrum. Under this condition, boosting can inflate the prior variances (13) along the right directions. For instance, if the impulse response energy is located at low frequencies, as ν increases boosting will amplify the low pass nature of the regularizer. This can significantly improve the estimate if the input is also low pass. 3.2 Hyperparameter estimation In the classical scheme described in section 2.1, ν is an iteration counter that only takes integer values, and the boosting scheme is sequential: to obtain the estimate ŷ(ν = m), one has to solve m optimization problems. Using (8) and (11), we can interpret ν as a kernel hyperparameter, and let it take real values. In the following we estimate both the scale factor λ and ν from the data, and restrict the range of ν to ν ≥ 1. The resulting boosting approach estimates (λ, ν) by minimizing fit measures such as cross validation or SURE (Hastie et al., 2001a). In particular, this accelerates the tuning procedure, as it requires solving a single problem instead of multiple boosting iterations. Consider estimating (λ, ν) using the SURE method. Given σ2 (e.g. using an unbiased estimator), choose (λ̂, ν̂) = arg min λ≥0,ν≥1 ‖y − ŷ(ν)‖2 + 2σ2trace(Sλ,ν). (14) Straightforward computations show that, for the cost of a single SVD, problem (14) simpli- fies to (λ̂, ν̂) = arg min λ≥0,ν≥1 n∑ i=1 z2i σ 4ν( λd2i + σ 2 )2ν + 2σ2n− n∑ i=1 2σ2ν+2 (λd2i + σ 2)ν , (15) which is a smooth 2-variable problem over a box, and can be easily optimized. We can also extract some useful information on the nature of the optimization problem (15). In fact, denoting J the objective, we have ∂J ∂ν = 2 n∑ i=1 log(αi)z 2 i α 2ν i − 2σ2 n∑ i=1 log(αi)α ν i = 2 n∑ i=1 log(αi)α ν i (z 2 i α ν i − σ2) , (16) 6 Boosting as a Kernel-Based Method 0 5 10 15 20 25 30 35 40 45 50 -1.2 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 True Ridge Boosting Figure 1: True signal (thick red line), Ridge estimate (solid blue) and Boosting estimate (dashed black) obtained in the first Monte Carlo run. The system impulse re- sponse is a low pass signal. where we have defined αi := σ2 λd2i+σ 2 . Simple considerations on the sign of the derivative then show that • if λ < min i=1,...,n z2i − σ2 d2i , (17) then ν̂ = +∞. This means that we have chosen a learner so weak that SURE suggests an infinite number of boosting iterations as optimal solution; • if λ > max i=1,...,n z2i − σ2 d2i , (18) then ν̂ = 1. This means that the weak learner is instead so strong that SURE suggests not to perform any boosting iterations. 3.3 Numerical illustration We illustrate our insights using a numerical experiment. Consider (2), where θ ∈ R50 represents the input to a discrete-time linear system. In particular, the signal is taken from (Wahba, 1990) and displayed in Fig. 1 (thick red line). The system is represented by the regression matrix U ∈ R200×50 whose components are realizations of either white noise or low pass filtered white Gaussian noise with normalized band [0, 0.95]. The measurement noise is white and Gaussian, with variance assumed known and set to the variance of the noiseless output divided by 10. We use a Monte Carlo of 100 runs to compare the following two estimators • Boosting: boosting estimator with K set to the identity matrix and with (λ, ν) estimated using the SURE strategy (14). 7 Aravkin, Bottegal and Pillonetto Ridge Boosting 70 72 74 76 78 80 82 84 86 88 90 Fits - WN input Ridge Boosting 30 40 50 60 70 80 90 Fits - LP input Figure 2: Boxplot of the percentage fits obtained by Ridge regression and Boosting, using SURE to estimate hyperparameters; system impulse response is white noise (left) and low pass (right). • Ridge: ridge regression (which corresponds to boosting with ν fixed to 1). Fig. 2 displays the box plots of the 100 percentage fits of θ, 100 ( 1− ‖θ−θ̂‖‖θ‖ ) , obtained by Boosting and Ridge. When the entries of U are white noise (left panel) one can see that the two estimators have similar performance. When the entries of U are filtered white noise (right panel) Boosting performs significantly better than Ridge. Furthermore, 36 out of the 100 fits achieved by Boosting under the white noise scenario are lower than those obtained adopting a low pass U , which is surprising since the conditioning of latter problem is much worse. The reasons are those previously described. The unknown θ represents a smooth signal. In Bayesian terms, setting K to the identity matrix corresponds to modeling it as white noise, which is a poor prior. If the nature of U is low pass, the energy of the d2i are more concentrated at low frequencies. So, as ν increases, Boosting can inflate the prior variances associated to the low-frequency components of θ. The prior variances associated to high-frequencies induce low SNRi, so that they increase slowly with ν. This does not happen in the white noise case, since the random variables d2i have similar distributions. Hence, the original white noise prior for θ can be significantly refined only in the low pass context: it is reshaped so as to form a regularizer, inducing more smoothness. Fig. 1 shows this effect by plotting estimates from Ridge and Boosting in a Monte Carlo run where U is low pass. 4. Boosting algorithms for general loss functions and RKHSs In this section, we combine the boosting kernel with piecewise linear-quadratic (PLQ) losses to obtain tractable algorithms for more general regression and classification problems. We also consider estimation in Reproducing Kernel Hilbert (RKHS) spaces. 8 Boosting as a Kernel-Based Method (a) quadratic (b) huber (d) hinge (e) quantile (f) vapnik (h) elastic net Figure 3: Six common piecewise-linear quadratic losses. 4.1 Boosting kernel-based estimation with general loss functions In the previous sections, the boosting kernel was derived using regularized least squares (3) as the weak learner. The sequence of resulting linear estimators then led to a closed form expression for Pλ,ν . Now, we consider a kernel-based weak learner (1), based on a general (convex) penalty V. Important examples include Vapnik’s epsilon insensitive loss (Fig. 3f) used in support vector regression (Vapnik, 1998; Hastie et al., 2001b; Schölkopf et al., 2000; Schölkopf and Smola, 2001), hinge loss (Fig. 3d) used for classification (Evgeniou et al., 2000; Pontil and Verri, 1998; Schölkopf et al., 2000), Huber and quantile huber (Fig 3b,e), used for robust regression(Huber, 2004; Maronna et al., 2006; Bube and Nemeth, 2007; Zou and Yuan, 2008; Koenker and Geling, 2001; Koenker, 2005; A. Aravkin et al., 2014), and elastic net (Fig. 3f), a sparse regularizer that also finds correlated predictors (Zou and Hastie, 2005b,a; Li and Lin, 2010; De Mol et al., 2009). The resulting boosting scheme is computationally expensive: ŷ(ν = m) requires solving a sequence of m optimization problems, each of which must be solved iteratively. In addition, since the estimators ŷ(ν) are no longer linear, deriving a boosting kernel is no longer straightforward. We combine general loss V with the regularizer induced by the boosting kernel from the linear case to define a new class of kernel-based boosting algorithms. More specifically, given a kernel K, let V DV T be the SVD of UKUT . If Pλ,ν is invertible, the boosting output estimate is ŷ(ν) = Uθ̂(ν) where θ̂(ν) = arg min θ V(y − Uθ) + σ2θTUTP−1λ,νUθ = arg min θ { V(y − Uθ) + θTUTV [( λD + σ2I σ2 )ν − I ]−1 V TUθ } , (19) where the last line is obtained using (11). Note, here and also in the reformulations below, that the solution depends on λ and σ2 only through the ratio γ = σ2/λ. If Pλ,ν is not invertible, the following two strategies can be adopted. 9 Aravkin, Bottegal and Pillonetto Approach I: We use (11) to obtain the factorization Pλ,ν = σ 2Aλ,νA T λ,ν , where Aλ,ν is full column rank and contains the columns of the matrix Aλ,ν = V [( λD + σ2I σ2 )ν − I ]1/2 associated to the di > 0. Then, the output estimate is ŷ(ν) = Aλ,ν â(ν) with â(ν) = arg min a { V(y −Aλ,νa) + aTa } . (20) The estimate of θ is then given by θ̂ = U †λ,ν ŷ(ν), where U † λ,ν is the pseudo-inverse of Uλ,ν . One advantage of the formulation (20) is that the evaluation of Aλ,ν for different λ and ν is efficient. Approach II: Define the matrix Bλ,ν = UPλ,νU T . Then, it is easy to see that another representation for the output estimate is ŷ(ν) = Bλ,ν b̂(ν) with b̂(ν) = arg min b { V(y −Bλ,νb) + bTBλ,νb } . (21) The new class of boosting kernel-based estimators defined by (20) or (21) keeps the advantages of boosting in the quadratic case. In particular, the kernel structure can decrease bias along directions less exposed to noise. The use of a general loss V allows a range of applications, with e.g. penalties such as Vapnik and Huber, guarding against outliers in the training set. Finally, the algorithm has clear computational advantages over the classic scheme described in Section 2.1. Whereas in the classic approach, ŷ(ν = m) require solving m optimization problems, in the new approach, given any positive λ and ν ≥ 1, the prediction ŷ(ν = m) is obtained by solving the single convex optimization problem (19). This is illustrated in Section 5. 4.2 New boosting algorithms in RKHSs We now show how the new class of boosting algorithms can be extended to the context of regularization in RKHSs. We start with `2 Boost in RKHSs. Assume that we want to reconstruct a function from n sparse and noisy data yi collected on input locations xi taking values on the input space X . Our aim now is to allow the function estimator to assume values in infinite-dimensional spaces, introducing suitable regularization to circumvent ill-posedness, e.g. in terms of function smoothness. For this 10 Boosting as a Kernel-Based Method purpose, we use K denote a kernel function K : X × X → R which captures smoothness properties of the unknown function. We can then use `2 Boost, with weak learner argmin f∈H n∑ i=1 Vi(yi − f(xi)) + γ‖f‖2H, (22) where Vi is a generic convex loss and H is the RKHS induced by K with norm denoted by ‖ · ‖H. From the representer theorem of Schölkopf et al. (2001), the solution of (22) is∑n i=1 ĉiK(xi, ·) where the ĉi are the components of the column vector argmin c∈Rn n∑ i=1 Vi(yi −Ki,·c) + γcTKc, (23) and K is the kernel (Gram) matrix, with Ki,j = K(xi, xj) and Ki,· is the i-th row of K. Using (23), we extend the boosting scheme from section 2.1 with (22) as the weak learner. In particular, repeated applications of the representer theorem ensure that, for any value of the iteration counter ν, the corresponding function estimate belongs to the subspace spanned by the n kernel sections K(xi, ·). Hence, `2 Boosting in RKHS can be summarized as follows. Boosting scheme in RKHS: 1. Set ν = 1. Solve (23) to obtain ĉ and f̂ for ν = 1, call them ĉ(1) and f̂(·, 1). 2. Update c by solving (23) with the current residuals as the data vector: ĉ(ν + 1) = ĉ(ν) + argmin c∈Rn n∑ i=1 Vi(yi −Kiĉ(ν)−Kic) + γcTKc, and set the new estimated function to f̂(·, ν + 1) = n∑ i=1 ĉi(ν + 1)K(xi, ·). 3. Increase ν by 1 and repeat step 2 for a prescribed number of iterations. There is a fundamental computational drawback related to this scheme which we have already encountered in the previous sections. To obtain f̂(·, ν) we need to solve ν opti- mization problems, each of them requiring an iterative procedure. Now, we define a new computationally efficient class of regularized estimators in RKHS. The idea is to obtain the expansion coefficients of the function estimate through the new boosting kernel. Letting γ = σ2/λ and Pλ = λK, with K the kernel matrix, define the boosting kernel Pλ,ν as in (8). Then, we can first solve b̂(ν) = arg min b { V(y − Pλ,νb) + bTPλ,νb } , (24) with V defined as the sum of the Vi. Then, we compute c̃ = K†ỹ(ν) with ỹ(ν) = Pλ,ν b̂(ν), 11 Aravkin, Bottegal and Pillonetto and the estimated function becomes f̂(·, ν) = n∑ i=1 c̃i(ν)K(xi, ·). Note that the weights c̃(ν) coincide with ĉ(ν) only when the Vi are quadratic. Neverthe- less, given any loss, (24) preserves all advantages of boosting outlined in the linear case. Furthermore, as in the finite-dimensional case, given any ν and kernel hyperparameter, the estimator (24) can compute c̃(ν) by solving a single problem, rather than iterating the boosting scheme. Classification with the hinge loss. Another advantage related to the use of the boost- ing kernel w.r.t. the classical boosting scheme arises in the classification context. Classifi- cation tries to predict one of two output values, e.g. 1 and -1, as a function of the input. `2 Boost could be used using the residual yi − f(xi) as misfit, e.g. equipping the weak learner (22) with the quadratic or the `1 loss. However, in this context one often prefers to use the margin mi = yif(xi) on an example (xi, yi) to measure how well the available data are classified. For this purpose, support vector classification is widely used (Schölkopf and Smola, 2002). It relies on the hinge loss Vi(yi, f(xi)) = |1− yif(xi)|+ = { 0, m > 1 1−m, m ≤ 1 , m = yif(xi), which gives a linear penalty whenm < 1. Note that this loss assumes yi ∈ {1,−1}. However, the classical boosting scheme applies the weak learner (22) repeatedly, and residuals will not be binary for ν > 1. This means that `2 Boost cannot be used for the hinge loss. This limitation does not affect the new class of boosting-kernel based estimators: support vector classification can be boosted by plugging in the hinge loss into (24): b̂(ν) = arg min b n∑ i=1 |1− yi[Pλ,νb]i|+ + bTPλ,νb, (25) where we have used [Pλ,νb]i to denote the i-th component of Pλ,νb. 5. Numerical Experiments 5.1 Boosting kernel regression: temperature prediction real data To test boosting on real data, we use a case study in thermodynamic modeling of buildings. Eight temperature sensors produced by Moteiv Inc were placed in two rooms of a small two-floor residential building of about 80 m2 and 200 m3. The experiment lasted for 8 days starting from February 24th, 2011; samples were taken every 5 minutes. A thermostat controlled the heating systems and the reference temperature was manually set every day depending upon occupancy and other needs. The goal of the experiment is to assess the predictive capability of models built using kernel-based estimators. We consider Multiple Input-Single Output (MISO) models. The temperature from the first node is the output (yi) and the other 7 represent the inputs (u j i , j = 1, .., 7). The 12 Boosting as a Kernel-Based Method Hours 0 2 4 6 8 P re d ic ti o n f it 76 78 80 82 84 86 88 90 92 94 Boosting SS SS Hours 0 10 20 30 40 T e m p e ra tu re C ( d e v ia ti o n ) -1 -0.5 0 0.5 1 1.5 2 2.5 3 3.5 30 min ahead prediction Test data Boosting prediction Figure 4: Left: prediction fits obtained by the stable spine estimator (SS) and by Boost- ing equipped with the stable spline kernel (Boosting SS). Right: 30-min ahead temperature prediction from Boosting SS on a portion of the test set. measurements are split into a training set of size Nid = 1000 and a test set of size Ntest = 1500. The notation ytest indicates the test data, which is used to test the ability of our estimator to predict future data. Data are normalized so that they have zero mean and unit variance before identification is performed. The model predictive power is measured in terms of k-step-ahead prediction fit on ytest, i.e. 100× \\uf8eb\\uf8ed1− √√√√Ntest∑ i=k (ytesti − ŷi|i−k)2/ √√√√Ntest∑ i=k (ytesti ) 2 \\uf8f6\\uf8f8 . We consider ARX models of the form yi = (g 1 ⊗ y)i + 7∑ j=1 (gj+1 ⊗ uj)i + vi, where ⊗ denotes discrete-time convolution and the {gj} are 8 unknown one-step ahead predictor impulse responses, each of length 50. Note that when such impulse responses are known, one can use them in an iterative fashion to obtain any k-step ahead prediction. We can stack all the {gj} in the vector θ and form the regression matrix U with the past outputs and the inputs so that the model becomes y = Uθ + v. Then, we consider the following two estimators: • Boosting SS: this estimator regularizes each gj introducing information on its smooth- ness and exponential decay by the stable spline kernel (Pillonetto and De Nicolao, 2010). In particular, let P ∈ R50×50 with (i, j) entry αmax(i,j), 0 ≤ α < 1. Then, we recover θ by the boosting scheme (20) with K = blkdiag(P, . . . , P ), and V set to the quadratic loss. Note that the estimator contains the three unknown hyperparameters ν, α and γ = σ2/λ. To estimate them, the training set is divided in half and hold-out cross validation is used. 13 Aravkin, Bottegal and Pillonetto • Classical Boosting SS: the same as above except that ν can assume only integer values. • SS: this is the stable spline estimator described in (Pillonetto and De Nicolao, 2010) (and corresponds to Boosting SS with ν = 1) with hyperparameters obtained via marginal likelihood optimization. For Boosting SS, we obtained γ = 0.02, α = 0.82 and ν = 1.42; note that it is not an integer. For Classical Boosting SS, we obtained γ = 0.03, α = 0.79 and ν = 1. In practice, this estimator gives the same results achieved by SS so that our discussion below just compares the performance of Boosting SS and SS. The left panel of Fig. 4 shows the prediction fits, as a function of the prediction horizon k, obtained by Boosting SS and SS. Note that the non-integer ν gives an improvement in performance. This means that in this experiment using a continuous ν improves also over the classical boosting. The right panel of Fig. 4 shows sample trajectories of half-hour-ahead boosting prediction on a part of the test set. 5.2 Boosting kernel regression using the `1 loss: Real data water tank system identification Time 0 100 200 300 400 500 600 -4 -2 0 2 4 6 8 10 12 14 Training set Time 0 50 100 150 200 250 -1.5 -1 -0.5 0 0.5 1 1.5 2 Test set Boosting simulation Figure 5: Left: training set. Right: test set simulation from Boosting SS with `1 loss. We test our new class of boosting algorithms on another real data set obtained from a water tank system (see also Bottegal et al. (2016)). In this example, a tank is fed with water by an electric pump. The water is drawn from a lower basin, and then flows back through a hole in the bottom of the tank. The system input is the voltage applied, while the output is the water level in the tank, measured by a pressure sensor at the bottom of the tank. The setup represents a typical control engineering scenario, where the experimenter is interested in building a mathematical model of the system in order to predict its behavior and design a control algorithm (Ljung, 1999). To this end, input/output samples are collected every second, comprising almost 1000 pairs that are divided into a training and test set. The signals are de-trended, removing their means. The training and test outputs are shown in the left and right panel of Fig. 5. One can see that the second part of the training data 14 Boosting as a Kernel-Based Method are corrupted by outliers caused by pressure perturbations in the tank; these are due to air occasionally being blown into the tank. Our aim is to understand the predictive capability of the boosting kernel even in presence of outliers. We consider a FIR model of the form yi = (g ⊗ u)i + vi, where the unknown vector g ∈ R50 contains the impulse response coefficients. It is estimated using a variation of the estimator Boosting SS described in the previous section: while the stable spline kernel is still employed to define the regularizer, the key difference is that V in (20) is now set to the robust `1 loss. The hyperparameter estimates obtained using hold-out cross validation are γ = 17.18, α = 0.92 and ν = 1.7. The right panel of Fig. 5 shows the boosting simulation of the test set. The estimate from Boosting SS predicts the test set with 76.2% fit. Using the approach V equal to the quadratic loss, the test set fit decreases to 57.8%. 5.3 Boosting in RKHSs: Classification problem Consider the problem described in Section 2 of (Hastie et al., 2001a). Two classes are introduced, each defined by a mixture of Gaussian clusters; the first 10 means are generated from a Gaussian N ([1 0]T , I) and remaining ten means from N ([0 1]T , I) with I the identity matrix. Class labels 1 and −1 corresponding to the clusters are generated randomly with probability 1/2. Observations for a given label are generated by picking one of the ten means mk from the correct cluster with uniform probability 1/10, and drawing an input location from N (mk, I/5). A Monte Carlo study of 100 runs is designed. At any run, a new data set of size 500 is generated, with the split given by 50% for training and 25% each for validation and testing. The validation set is used to estimate through hold-out cross-validation the unknown hyperparameters, in particular the boosting parameter ν. Performance for a given run is quantified by computing percentage of data correctly classified. We compare the performance of the following two estimators: • Boosting+`1 loss: this is the boosting scheme in RKHS illustrated in the previous section (ν may assume only integer values) with the weak learner (22) defined by the Gaussian kernel K(x, a) = exp(−10|x− a|2), | · | = Euclidean norm setting each Vi to the `1 loss and using γ = 1000. • Boosting kernel+`1 loss: this is the estimator using the new boosting kernel. The latter is defined by the kernel matrix built using the same Gaussian kernel reported above, with σ2 = 1, λ = 0.001 so that one still has γ = 1000. The function estimate is achieved solving (24) using the `1 loss. Note that the two estimators contain only one unknown parameter, i.e. ν which is estimated by the cross validation strategy described above. The top left panel of Fig. 6 compares their performance. Interestingly, results are very similar, see also Table 1. This supports the fact that the boosting kernel can include classical boosting features in the 15 Aravkin, Bottegal and Pillonetto Boosting+`1 Boosting kernel+`1 Boosting SVC SVC 78.91 % 79.15 % 79.73 % 78.12 % Table 1: Average percentage classification fit estimation process. In this example, the difference between the two methods is mainly in their computational complexity. In particular, the top right panel of Fig. 6 reports some cross validation scores as a function of the boosting iterations counter ν for the classical boosting scheme. The score is linearly interpolated, since ν can assume only integer values. On average, during the 100 Monte Carlo runs the optimal value corresponds to ν = 340, so on average, problems (22) must be solved 340 times. After obtaining the estimate of ν, to obtain the function estimate using the union of the training and validation data, another 340 problems must be solved. In contrast, the boosting kernel used in (24) does not require repeated optimization of the weak learner. Using a golden section search,estimating ν by cross validation on average requires solving 20 problems of the form (24). Once ν is found, only one additional optimiza- tion problem must be solved to obtain the function estimate. Summarizing, in this example the boosting kernel obtains results similar to those achieved by classical boosting, but re- quires solving only 20 optimization problems rather than nearly 700. The computational times of the two approaches are reported in the bottom panel of Fig. 6. Table 1 also shows the average fit obtained by other two estimators. The first estimator is denoted by Boosting SVC: it coincides with Boosting kernel+`1 loss, except that the hinge loss replaces the `1 loss in (24). The other one is SVC and corresponds to the classical support vector classifier. It uses the same Gaussian kernel defined above with the regularization parameter γ determined via cross validation on a grid containing 20 logarithmically spaced values on the interval [0.01, 100]. One can see that the best results are obtained by boosting support vector classification. Recall also that the hinge loss cannot be adopted using the classical boosting scheme as discussed at the end of the previous section. 5.4 Boosting in RKHSs: Regression problem Consider now a regression problem where only smoothness information is available to recon- struct the unknown function from sparse and noisy data. As in the previous example, our aim is to illustrate how the new class of proposed boosting algorithms can solve this problem using a RKHS with a great computational advantage w.r.t. the traditional scheme. For this purpose, we just consider a classical benchmark problem where the unknown map is the Franke’s bivariate test function f given by the weighted sum of four exponentials (Wahba, 1990). Data set size is 1000 and is generated as follows. First, 1000 input locations xi are drawn from a uniform distribution on [0, 1]× [0, 1]. The data are divided in the same way described in the classification problem. The outputs in the training and validation data are yi = f(xi) + vi 16 Boosting as a Kernel-Based Method 55 60 65 70 75 80 85 90 95 100 55 60 65 70 75 80 85 90 95 100 Classification prediction fit Bo os tin g + l 1 lo ss Boosting kernel + l1 loss 0 100 200 300 400 500 600 700 7 8 9 10 11 12 13 14 15 Boosting iteration counter C V sc or e by B oo st in g + l 1 lo ss 0.5 1 1.5 2 2.5 3 Boosting kernel + l 1 loss 20 30 40 50 60 70 80 90 100 B o o s ti n g + l 1 l o s s Computational time (seconds) Figure 6: Classification problem Top Left Fits obtained by the new boosting kernel (x-axis) vs fits obtained by the classical boosting scheme (y-axis). Both the estimators use the `1 loss. Top Right Some cross validation scores computed using the classical boosting scheme equipped with the `1 loss as a function of the boosting iteration counter ν. Each curve corresponds to a different run. Bottom Computational times to solve a classification problem needed by the new boosting kernel (x-axis) and by the classical boosting scheme (y-axis). where the errors vi are independent, with distribution given by the mixture of Gaussians 0.9N (0, 0.12) + 0.1N (0, 1). The test outputs ytesti are instead given by noiseless outputs f(x test i ). A Monte Carlo study of 100 runs is considered, where a new data set is generated at any run. The test fit is computed as 100 ( 1− |y test − ŷtest| |ytest −mean(ytest)| ) , where ŷtest is the test set prediction. Note that the mixture noise can model the effect of outliers which affect, on average, 1 out of 10 outputs. This motivates the use of the robust `1 loss. Hence, the function is still 17 Aravkin, Bottegal and Pillonetto Boosting+`1 Boosting kernel+`1 Gaussian kernel+`1 76.62 % 76.75 % 75.19 % Table 2: Average percentage regression fit reconstructed by Boosting+`1 loss and Boosting kernel+`1 loss which are implemented exactly in the same way as previously described. Fig. 7 displays the results with the same rationale adopted in Fig. 6. The fits are close each other but, at any run, the classical boosting scheme requires solving hundreds of optimization problems, while the boosting kernel-based approach needs to solve around 15 problems on average. The computational times of the two approaches are reported in the bottom panel of Fig. 7. Finally, Table 2 reports the average fits including those achieved by Gaussian kernel+`1 loss, which is implemented as the estimator SVC described in the previous section except that the hinge loss is replaced by the `1 loss. The best results are achieved by boosting kernel with `1. 6. Conclusion In this paper, we presented a connection between boosting and kernel-based methods. We showed that in the context of regularized least-squares, boosting with a weak learner can be interpreted using a boosting kernel. This connection was used for three main applications: (1) providing insight into boosting estimators and when they can be effective; (2) determin- ing schemes for hyperparameter estimation using the kernel connection and (3) proposing a more general class of boosting schemes for general misfit measures, including `1, Huber and Vapnik, which can use also RKHSs as hypothesis spaces. The proposed approach combines generality with computational efficiency. In contract to the classic boosting scheme, treating boosting iterations ν as a continuous hyperparam- eter may improve prediction capability. Real data support the use of these generalized schemes in practice. Indeed, in some real experiments we obtained ν = 1.42 as estimate im- proving on the classic scheme. In addition, this new viewpoint avoids sequential solutions. This turns out a particularly strong advantage for boosting using general losses V, as each boosting run would itself require an iterative algorithm. This has been outlined also in the RKHS setting: the boosting kernel allows to obtain results similar (or also better) than the classical boosting scheme dramatically reducing the computational cost. References A. Aravkin, P. Kambadur, A.C. Lozano, and R. Luss. Orthogonal matching pursuit for sparse quantile regression. In Data Mining (ICDM), International Conference on, pages 11–19. IEEE, 2014. R. Avnimelech and N. Intrator. Boosting regression estimators. Neural computation, 11(2): 499–520, 1999. 18 Boosting as a Kernel-Based Method 55 60 65 70 75 80 85 90 55 60 65 70 75 80 85 90 Regression prediction fit Bo os tin g + l 1 lo ss Boosting kernel + l1 loss 0 50 100 150 200 250 12 13 14 15 16 17 18 Boosting iteration counter C V sc or e by B oo st in g + l 1 lo ss 0 0.5 1 1.5 2 Boosting kernel + l 1 loss 0 5 10 15 20 25 30 35 B o o s ti n g + l 1 l o s s Computational time (seconds) Figure 7: Regression problem Top Left Fits obtained by the new boosting kernel (x-axis) vs fits obtained by the classical boosting scheme (y-axis). Both the estimators use the `1 loss. Top Right Some cross validation scores computed using the classical boosting scheme equipped with the `1 loss as a function of the boosting iteration counter ν. Bottom Computational times to solve a regression problem needed by the new boosting kernel (x-axis) and by the classical boosting scheme (y-axis). A. Bissacco, M.-H. Yang, and S. Soatto. Fast human pose estimation using appearance and motion via multi-dimensional boosting regression. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8. IEEE, 2007. G. Bottegal, A.Y. Aravkin, H. Hjalmarsson, and G. Pillonetto. Robust EM kernel-based methods for linear system identification. Automatica, 67:114–126, 2016. L. Breiman. Arcing classifier (with discussion and a rejoinder by the author). The annals of statistics, 26(3):801–849, 1998. K.P. Bube and T. Nemeth. Fast line searches for the robust solution of linear systems in the hybrid `1/`2 and huber norms. Geophysics, 72(2):A13–A17, 2007. 19 Aravkin, Bottegal and Pillonetto P. Bühlmann and T. Hothorn. Boosting algorithms: Regularization, prediction and model fitting. Statistical Science, pages 477–505, 2007. P. Bühlmann and B. Yu. Boosting with the L2 loss: regression and classification. Journal of the American Statistical Association, 98(462):324–339, 2003. X. Cao, Y. Wei, F. Wen, and J. Sun. Face alignment by explicit shape regression. Interna- tional Journal of Computer Vision, 107(2):177–190, 2014. M. Champion, C. Cierco-Ayrolles, S. Gadat, and M. Vignes. Sparse regression and support recovery with L2-boosting algorithms. Journal of Statistical Planning and Inference, 155: 19–41, 2014. C. De Mol, E. De Vito, and L. Rosasco. Elastic-net regularization in learning theory. Journal of Complexity, 25(2):201–230, 2009. T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and support vector ma- chines. Advances in Computational Mathematics, 13:1–150, 2000. W. Fan, S.J. Stolfo, and J. Zhang. The application of adaboost for distributed, scalable and on-line learning. In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 362–366. ACM, 1999. Y. Freund and R.E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences, 55(1):119–139, 1997. Y. Freund, R. Schapire, and N. Abe. A short introduction to boosting. Journal-Japanese Society For Artificial Intelligence, 14(771-780):1612, 1999. J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28 (2):337–407, 2000. M.H. Hansen and B. Yu. Model selection and the principle of minimum description length. Journal of the American Statistical Association, 96(454):746–774, 2001. T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning, volume 1. Springer series in statistics Springer, Berlin, 2001a. T.J. Hastie, R.J. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Data Mining, Inference and Prediction. Springer, Canada, 2001b. P. J. Huber. Robust Statistics. John Wiley and Sons, 2004. C.M. Hurvich, J.S. Simonoff, and C.-L. Tsai. Smoothing parameter selection in nonpara- metric regression using an improved Akaike information criterion. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(2):271–293, 1998. R. Koenker. Quantile Regression. Cambridge University Press, 2005. 20 Boosting as a Kernel-Based Method R. Koenker and O. Geling. Reappraising medfly longevity: A quantile regression survival analysis. Journal of the American Statistical Association, 96:458468, 2001. A. Lemmens and C. Croux. Bagging and boosting classification trees to predict churn. Journal of Marketing Research, 43(2):276–286, 2006. Q. Li and N. Lin. The bayesian elastic net. Bayesian Analysis, 5(1):151–170, 2010. L. Ljung. System Identification, Theory for the User. Prentice Hall, 1999. R.A. Maronna, D. Martin, and V.J. Yohai. Robust Statistics. Wiley Series in Probability and Statistics. Wiley, 2006. G. Pillonetto and G. De Nicolao. A new kernel-based approach for linear system identifi- cation. Automatica, 46(1):81–93, 2010. M. Pontil and A. Verri. Properties of support vector machines. Neural Computation, 10: 955–974, 1998. R.E. Schapire. The strength of weak learnability. Machine learning, 5(2):197–227, 1990. R.E. Schapire. The boosting approach to machine learning: An overview. In Nonlinear estimation and classification, pages 149–171. Springer, 2003. R.E. Schapire and Y. Freund. Boosting: Foundations and algorithms. MIT press, 2012. B. Schölkopf and A. J. Smola. Learning with Kernels: Support Vector Machines, Regu- larization, Optimization, and Beyond. (Adaptive Computation and Machine Learning). MIT Press, 2001. B. Schölkopf and A.J. Smola. Learning with kernels: support vector machines, regulariza- tion, optimization, and beyond. MIT press, 2002. B. Schölkopf, A.J. Smola, R.C. Williamson, and P.L. Bartlett. New support vector algo- rithms. Neural Computation, 12:1207–1245, 2000. B. Schölkopf, R. Herbrich, and A. J. Smola. A generalized representer theorem. Neural Networks and Computational Learning Theory, 81:416–426, 2001. D.P. Solomatine and D.L. Shrestha. AdaBoost. RT: a boosting algorithm for regression problems. In Neural Networks, 2004. Proceedings. 2004 IEEE International Joint Con- ference on, volume 2, pages 1163–1168. IEEE, 2004. V.N. Temlyakov. Weak greedy algorithms. Advances in Computational Mathematics, 12 (2-3):213–227, 2000. P. Tokarczyk, J.D. Wegner, S. Walk, and K. Schindler. Features, color spaces, and boosting: New insights on semantic classification of remote sensing images. IEEE Transactions on Geoscience and Remote Sensing, 53(1):280–295, 2015. 21 Aravkin, Bottegal and Pillonetto Z. Tu. Probabilistic boosting-tree: Learning discriminative models for classification, recog- nition, and clustering. In Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, volume 2, pages 1589–1596. IEEE, 2005. G. Tutz and H. Binder. Boosting ridge regression. Computational Statistics & Data Anal- ysis, 51(12):6044–6059, 2007. V. Vapnik. Statistical Learning Theory. Wiley, New York, NY, USA, 1998. P. Viola and M. Jones. Fast and robust classification using asymmetric adaboost and a detector cascade. Advances in Neural Information Processing System, 14, 2001. G. Wahba. Spline models for observational data. SIAM, Philadelphia, 1990. J. Zhu, H. Zou, S. Rosset, and T. Hastie. Multi-class adaboost. Statistics and its Interface, 2(3):349–360, 2009. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society, Series B, 67:301–320, 2005a. H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320, 2005b. H. Zou and M. Yuan. Regularized simultaneous model selection in multiple quantiles re- gression. Computational Statistics & Data Analysis, 52(12):5296–5304, 2008. 22 1 Introduction 2 Boosting as a kernel-based method 2.1 Boosting: notation and overview 2.2 Using regularized least squares as weak learner 2.3 The boosting kernel 3 Consequences 3.1 Insights on the nature of boosting 3.2 Hyperparameter estimation 3.3 Numerical illustration 4 Boosting algorithms for general loss functions and RKHSs 4.1 Boosting kernel-based estimation with general loss functions 4.2 New boosting algorithms in RKHSs 5 Numerical Experiments 5.1 Boosting kernel regression: temperature prediction real data 5.2 Boosting kernel regression using the 1 loss: Real data water tank system identification 5.3 Boosting in RKHSs: Classification problem 5.4 Boosting in RKHSs: Regression problem 6 '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['text'].loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}